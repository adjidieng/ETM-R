% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/etm_embed.R
\name{etm_embed}
\alias{etm_embed}
\title{Fit Word Embeddings to Text}
\usage{
etm_embed(data_path, output_name, dim_rho = 300, min_count = 2,
  iters = 50, window_size = 4, neg_samples = 10, num_cores = 25,
  algorithm = "skipgram")
}
\arguments{
\item{data_path}{Path to the corpus text file.}

\item{output_name}{File name for the output, which is the word embeddings text file.
Default is "embeddings.txt".}

\item{dim_rho}{Number of dimensions for rho, which is the number of
embedding representations of the vocabulary.  Default is 300 dimensions.}

\item{min_count}{Minimum term frequency (to define the vocabulary).  Default is 2.}

\item{iters}{Number of iterations.  Default is 50 iterations.}

\item{window_size}{The window size to determine context.  In other words,
the number of words surrounding the word of interest.  Default size is 4.}

\item{neg_samples}{The number of negative samples allowed.  In other words, the number
of "noise words" that are drawn.  Default is 10.}

\item{num_cores}{The number of CPU cores to use.  To automatically use the optimal
number of CPU cores available, use "detect".  Otherwise, specify an integer (e.g. 4).
The default number of cores is 25.}

\item{algorithm}{The word embedding algorithm to use.  "skipgram" for skip-grams,
or "cbow" for continuous bag of words.  Default is skip-gram.}
}
\description{
`etm_embed` fits word embeddings on your corpus using the popular neural network model
Word2Vec.  Choose between two word embedding algorithms: skip-grams, or continuous bag of words.
`etm_embed` produces a text file with the word embeddings.
}
\examples{
\dontrun{
etm_embed(data_path = "Desktop/corpus.txt",
output_name = "Desktop/embeddings.txt",
dim_rho = 300, iters = 50, window_size = 4, neg_samples = 10,
num_cores = "detect", algorithm = "skipgram")

etm_embed(data_path = "Desktop/news.txt", output_name = "Documents/embed_file.txt",
num_cores = 4, algorithm = "cbow")
}
}
